<p align="center" style="font-size:50px">
    <a href="https://github.com/lsw6684/ComputerScience">HOME</a>
</p>

***

<br />

# Natural Language Processing
- [Foundation](#foundation)
- [Machine Learning with scikit-learn](#machine-learning-with-scikit-learn)
- [Optimization](#optimization)
- [Probability](#probability)
- [Information Theory_Entropy](#information-theoryentropy)
<br />

## Foundation
**SciPy** : Scientific Computingìœ¼ë¡œ ìˆ˜í•™, ê³¼í•™, ê·¸ë¦¬ê³  ê³µí•™ ê´€ë ¨ëœ ì—°ì‚°ì„ ì§€ì›í•©ë‹ˆë‹¤. MATLABì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ Computing Toolì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
- **array**
    ```python
    import numpy as np
    # From a composite data (list or tuple, not set and dictionary)
    A = np.array([3, 29, 82])                   #ë¦¬ìŠ¤íŠ¸
    B = np.array(((3., 29, 82), (10, 18, 84)))  #íŠœí”Œ, "."ì€ float 64 ì˜ë¯¸. 
    C = np.array([[3, 29, 82], [10, 18, 84]], dtype=float)
    D = np.array([3, 29, 'choi'])
    E = np.array([[3], [29], [82]])

    # ndim = ì°¨ì›, size = element ê°œìˆ˜, shape = í–‰ë ¬. ì™œ 1ì´ ì•ˆë‚˜ì˜¬ê¹Œ
    # dtype = ë°ì´í„°íƒ€ì…. í•˜ë‚˜ë¥¼ ì„¤ì •í•´ ë†“ìœ¼ë©´ ëª¨ë‘ ë°”ë€ë‹¤ - homogenous
    print(A.ndim, A.size, A.shape, A.dtype)     # 1 3 (3,) int 32
    print(B.ndim, B.size, B.shape, B.dtype)     # 2 6 (2, 3) float64
    print(C.ndim, C.size, C.shape, C.dtype)     # 2 6 (2, 3) float64
    print(D.ndim, D.size, D.shape, D.dtype)     # 1 3 (3, 1) ìœ ë‹ˆì½”ë“œë¡œ ë‚˜ì˜¤ê³  
    print(D) # ['3' '29' 'choi']ì •ìˆ˜ë“¤ë„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    print(E.ndim, E.size, E.shape, E.dtype)     # 2 3 (3, 1) int32

    # Using initialization functions
    F = np.zeros((3, 2))            # Create a 3x2 array filled with 0 (default: float 64)
    G = np.ones((3, 2))             # Create a 2x3 array filled with 1
    H = np.eye(3, dtype=np.float32) # Create a 3x3 identity matrix (single-precision)
    I = np.empty((3, 2))            # zeros((3, 2)) but the elemets are 1
    J = np.empty((0, 9))            # [] width size of (0, 9). just space.
    K = np.arange(0, 1, 0.2)        # 0ì´ìƒ 1 ë¯¸ë§Œ, 0.2(step)ì°¨ì´ë¡œ ë‚˜ì—´
    L = np.linspace(0, 1, 5)        # 0ì´ìƒ 1 ì´í•˜, 5ê°œë¡œ ë‚˜ëˆ„ê¸°
    M = np. random.random((3, 2)) # == np.random.uniform(size=(3, 2)) cf. normal()    
    ```
- **Indexing and slicing**
<p align="center"><img src="images/indexingSlicing.png" width="650"></p>


- **Matplotlib** : 2ì°¨ì› ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. 3ì°¨ì›ë„ ì§€ì› í•˜ì§€ë§Œ, ë³„ë¡œ...
    ```python
    import matplotlib.pyplot as plt
    import numpy as np

    xs = np.arange(-4, 10, 0.001) # -4ì´ìƒ 10ë¯¸ë§Œê¹Œì§€ 0.001ê°„ê²©ìœ¼ë¡œ ìƒì„±
    ys = [0.1*x**3 - 0.8*x**2 - 1.5*x +5.4 for x in xs]
    yt = [-3.5*x + 7 for x in xs]

    fig = plt.figure()              # figureëŠ” ê·¸ë¦¼ ë‹¨ìœ„. plotì„ ê´€ë¦¬
    trim = fig.add_subplot(1, 1, 1) # subplotì€ plot 1ê°œë¥¼ ì˜ë¯¸. 1x1 ë¹„ìœ¨ë¡œ, ì²« ë²ˆì§¸ plot
    trim.set_aspect('equal')        # ì§€ê¸ˆì€ plotì´ 1ê°œ ë¿ì´ë¼ ë§ˆì§€ë§‰ ë§¤ê°œë³€ìˆ˜ëŠ” 1ë°–ì— ì•ˆë¨.

    plt.plot(xs, ys, color='r', label = 'y')
    plt.plot(xs, yt, color='b', label='tangent line')

    plt.grid()   # ê·¸ë¦¬ë“œ
    plt.legend() # ë¼ë²¨
    plt.show()   # í™”í¸ì— ì¶œë ¥.
    ```
<p align="center"><img src="images/matplotlib_basic.png" width="180"></p>

- **SymPy** : ìˆ˜í•™ì  ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
<p align="center"><img src="images/sympy_basic.png" width="600"></p>

<p align="center"><img src="images/factorize.png" width="700"></p>

- **NumPy** : ë‹¤ì°¨ì› ë²¡í„°, í–‰ë ¬ì„ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¹ ë¥¸ ì†ë„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. arrayí˜•íƒœì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ(ì†ë„â†‘ ë©”ëª¨ë¦¬â†“) ì²˜ë¦¬í•©ë‹ˆë‹¤.
    <p align="center"><img src="images/í–‰ë ¬.png" width="600"></p>
    Numpy.arrayëŠ” **homogeneous**, ì œì°¨í˜• ë°ì´í„° íƒ€ì…ë§Œ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    
- **Data types**

    ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    ```python
    profs = [
    'My name is Choi and my E-mail is sunglok@seoultech.ac.kr.',
    'My name is Kim and my e-mail address is jindae.kim@seoultech.ac.kr.'
    ]
    print(['e-mail' == prof for prof in profs ])        # [False, False] cf. matching
    print(['e-mail' in prof for prof in profs])         # [False, True] cf. searching
    print(['e-mail' in prof.lower() for prof in profs]) # [True, True] cf.
    print([prof.find('e-mail') for prof in profs])      # [-1, 22]
    print([prof endswith('.') for prof in profs])       # [True, True] cf. startswith
    ```
    - **fnmatch**
        - \* : matches everything
        - ? : matches any single character
        - [seq] : matches any character in seq
        - [!seq] : matches any character not in seq 
        
            **fnmatchëŠ”** ëŒ€/ì†Œë¬¸ìë¥¼ êµ¬ë³„í•˜ì§€ ì•Šê³ , **fnmatchcase**ë¥¼ ì‚¬ìš©í•´ì•¼ ëŒ€/ì†Œë¬¸ìë¥¼ êµ¬ë³„í•©ë‹ˆë‹¤.
        ```python
        import fnmatch

        profs = [
        'My name is Choi and my E-mail is sunglok@seoultech.ac.kr.',
        'My name is Kim and my e-mail address is jindae.kim@seoultech.ac.kr.'
        ]

        # for a single string
        print([fnmatch.fnmatch(prof, 'e-mail') for prof in profs])
        print([fnmatch.fnmatch(prof, '*e-mail*') for prof in profs])
        print([fnmatch.fnmatchcase(prof, '*e-mail*') for prof in profs])
        print([fnmatch.fnmatchcase(prof, '*[Ee]-mail*') for prof in profs])

        # for a list of strings
        print(fnmatch.filter(profs, '*e-mail*'))
        print(fnmatch.filter(profs, '*ch?i*'))        
        >>>
            [False, False]
            [True, True]
            [False, True]
            [True, True]
            ['My name is Choi and my E-mail is sunglok@seoultech.ac.kr.', 'My name is Kim and my e-mail address is jindae.kim@seoultech.ac.kr.']
            ['My name is Choi and my E-mail is sunglok@seoultech.ac.kr.']

        ```
- **ì •ê·œì‹ Regular Expression** <br />
searchë¥¼ ìœ„í•œ string íŒ¨í„´ì…ë‹ˆë‹¤. ë³µì¡í•œ ë¬¸ìì—´ì„ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, Pythonë§Œì˜ ê³ ìœ  ë¬¸ë²•ì´ ì•„ë‹ˆë¼ ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“  ê³³ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì •ê·œ í‘œí˜„ì‹ì„ ë°°ìš°ëŠ” ê²ƒì€ Pythonì„ ë°°ìš°ëŠ” ê²ƒê³¼ëŠ” ë˜ ë‹¤ë¥¸ ì˜ì—­ì˜ ê³¼ì œë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<p align="center"><img src="images/re1.png" width="1000"></p>

<p align="center"><img src="images/re2.png" width="500"></p>

<p align="center"><img src="images/re3.png" width="500"></p>

<p align="center"><img src="images/re4.png" width="1500"></p>

## Machine Learning with scikit-learn
- **Machine Learning**
    - ë°ì´í„°ë‚˜ ê²½í—˜ì„ í†µí•´ ìë™ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    - **Data-driven approaches** vs **rule/model-based approaches**
    - **ê·€ë‚©ë²• Induction** vs **ì—°ì—­ë²• deduction**
    <p align="center"><img src="images/induction_deduction.png" width="1000"></p>
- **ML procedure**
    1. Data acquisition, ë°ì´í„° ì¤€ë¹„
    2. Data preprocessing, ì „ì²˜ë¦¬(ë¼ë²¨ë§, ì •ê·œí™” ë“±)
    3. Feature selection and extraction, ìœ ì˜ë¯¸í•œ íŠ¹ì§• ë„ì¶œ
    4. Model and cost function selection (or design)
    5. Hyperparameter selection (learning rate, optimizer)
    6. Model **training**
    7. Model **testing**
- **ML approaches (with respect to ***the given data***)**
    - ì§€ë„ í•™ìŠµ Supervised learning - input, desired targetì´ í•¨ê»˜ ë¶€ì—¬ë©ë‹ˆë‹¤.
        - Weakly supervised learning : semi-supervised learning
    - ë¹„ì§€ë„ í•™ìŠµ Unsupervised learning - inputë§Œ ë¶€ì—¬ë©ë‹ˆë‹¤.
        - Clustering, Dimensional reduction
    - Reinforcement learning - feedback(reward/penalty)ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.
- **ML problem formulation**
    <p align="center"><img src="images/ml_fourmulations.png" width="900"></p>
- **ML Road Map - Choosing the right algorithm**
    <p align="center"><img src="images/roadMap.png" width="1400"></p> <br />
- **scikit-learn**
    - A Python-based open-source ML library
    - Built on NumPy, SciPy, and matplotlib
    - Included in Anaconda by default
    - Includes *example datasets* <br />

    ì‚¬ìš© ìˆœì„œ
    1. Instantiation, ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    2. Training(fit)
    3. testing(predict)
- ### SVM Support vector machine
    - ë°ì´í„°ê°€ ì ì„ ë•Œì™€ high-dimensionalì¼ ë•Œ ì ì ˆí•©ë‹ˆë‹¤.
    - Supervised learning models(ë³´í†µ classification and regression)
    - A non-probabilistic (deterministic), ë¹„í™•ë¥ ì  (ê²°ì •ë¡ ì ) binary linear classifier.
        - Finding the maximum-margin **ì´ˆí‰ë©´ hyperplane**(boundary - 2D, plane in 3D)
        - Optimization. ë‹¤ë¥¸ ì˜ì—­ìœ¼ë¡œ ê°€ë©´ ìŒìˆ˜ - ê° ì˜ì—­ì— ë§ê²Œ Maximize
    - **Kerner trick**ì„ ì´ìš©í•˜ë©´ non-linear classificationì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        - **Advantages**
        1. Effective in high-dimensional spaces
        2. when dimensions > samples
        3. Customizable by specifying kernel functions
        - **Disadvantages**
        1. probabilistic estimatesë¥¼ ë°”ë¡œ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        2. noisy data and large-scale dataì—ì„œ ì ì ˆí•œ ì‚¬ìš©ì´ ì–´ë µìŠµë‹ˆë‹¤.
- **How to get good results?**<br />
    ë‹µì´ ì—†ìŠµë‹ˆë‹¤. <br />
    Trial and error (more data and more computing power), but your intuition and experience are also important.
- **Cross-validation, CV** <br />
    trained modelì´ ë…ë¦½ì ì¸ data setìœ¼ë¡œ ì¼ë°˜í™” ë˜ëŠ” model evaluationì…ë‹ˆë‹¤.
    - **Exhaustive cross-validation**, ëª¨ë“  ì¡°í•©ì— ëŒ€í•˜ì—¬ ì ìš©
        - **Leave-p-out cross-validaion, LpOCV** : n-p data and test p data
        - **Leave-one-out cross-validation, LOOCV** : n times of trainings (p=1)
    - **Non-exhaustive corss-validation**, ê·¼ì‚¬ì ìœ¼ë¡œ ì¼ë¶€ ì¡°í•©ì— ëŒ€í•˜ì—¬ ì ìš©
        - **k-foldcross-validation** : LOOCVì—ì„œ Oneì´ í•œ ê°œì˜ ë°ì´í„°ê°€ ì•„ë‹Œ í•œ ê°œì˜ Unitì…ë‹ˆë‹¤.
- **Classification**
    - **Binary classification**
        - 2ê°€ì§€ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. (0 or 1, True or False)
        - e.g. ëŒ€í™”ë‚´ìš©ì—ì„œ ì£¼ì†Œ ì°¾ê¸°
        - AccuracyëŠ” imbalanced dataì— ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - **Confusion matrix**
        ![gd](./images/confusion%20matrix.png)
    - **Multiclass classification**
        - 3ê°œ ì´ìƒì¸ ë©€í‹°í´ë˜ìŠ¤
        ![gd](./images/ms.png)
    - [**SVM**](#svm-support-vector-machine)
    - **Decision tree**
        - if-then-else flowchart
        - ì¥ì  : white boxë¡œ ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆìœ¼ë©° modifyingì´ ì‰½ìŠµë‹ˆë‹¤.
        - ë‹¨ì  : Unstable, sensitive and inaccurate. <br />
        ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´**Random forest**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ê°œì˜ decision treeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - **Naive Bayes classifiers**
        - Naive assumptionì„ ê¸°ë°˜ìœ¼ë¡œ bayes' theoremì„ ì´ìš©í•©ë‹ˆë‹¤.
- **Regression**
    - **Linear regression** : scalar response(y)ì™€ observed variables(x) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ì„ í˜• approachì…ë‹ˆë‹¤.
- **Clustering**
    - **k-means clustering** : within-cluster sum of squares(WCSS)ë¥¼ ìµœì†Œí™” í•˜ëŠ” centroid(mean)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
- **Feature Extraction**
    - **Bag-of-words model, BoW** : ê°ê°ì˜ ë‹¨ì–´ë¡œ ìª¼ê°œê³  ìˆœì„œì™€ ìƒê´€ ì—†ì´ ë²¡í„°í™”í•©ë‹ˆë‹¤.<br />
        ![gd](./images/bow1.png)    ![gd](./images/bow2.png)
    - **TF-IDE weighting** : ë‹¨ìˆœí•œ countê°€ ì•„ë‹Œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.
        - **TF, term frequency** - document dë¥¼ ë‹¨ì–´ì˜ ë¹ˆë„ì— ëŒ€í•˜ì—¬ normalize ì‹œì¼œì¤ë‹ˆë‹¤.
    - **IDF, inverse document frequency** - in all documents D
    - **TF-IDF** = tf(t, d) x idf(t, D)

<br />

## Optimization
ìµœì í™”, Criterion(and constraints)ì— ê¸°ë°˜í•˜ì—¬ ê°€ì¥ ì í•©í•œ Elementë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.

- **Nonlinear Optimization**
    - ë¹„ì„ í˜• ìµœì í™”, NLP
    - Maxima, Minima or stationary pointsë¥¼ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
        - Object functions and constraints : nonlinearí•˜ë©° ê²°ê³¼ ê°’ì´ real-valuedì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.<br />
            Maximization : Profit/Utility/Fitness/Reward/...functions <br />
            Minimization : Loss/Cost/Penalty/...functions <br />
            ğŸ”Maximization and Minimization is dual.(**Minimization** is usually preferred)
    - **Gradient descent**
        - ê·¹ê°’ì— ì´ë¥¼ ë•Œê¹Œì§€ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ“ê°’ì´ ë‚®ì•„ì§€ëŠ”(ê°€íŒŒë¥¸) ë°©í–¥ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    - **Stochastic gradient descent, SGD**
        - randomìœ¼ë¡œ ì„ íƒëœ ë°ì´í„°ë¡œ ê³„ì‚°í•˜ì—¬ ëŒ€ëµì ì¸ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.
        <p align="center"><img src="images/gradient.png" width="500"></p>
    - **Newton's method**
        - ê¸°ìš¸ê¸°ê°€ ì•„ë‹Œ, í•œ ë²ˆ ë” ë¯¸ë¶„í•œ ê°’ì„ ì´ìš©í•©ë‹ˆë‹¤.
    - **Gauss-Newton method**
        - ì œê³±ê¼´ë¡œ ì£¼ì–´ì§„ í•¨ìˆ˜ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.

<br />

## Probability
Eventê°€ ì–¼ë§ˆë‚˜ ë°œìƒí•  ì§€, ì–´ë–¤ ëª…ì œì˜ ì°¸/ê±°ì§“ì¼ ì •ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- **Axioms**
    - For any event X, 0 <= P(X) : í™•ë¥  ê°’ì€ í•­ìƒ 0ì´ìƒì…ë‹ˆë‹¤.
    - Probability of the sample space S is P(S) = 1 : ëª¨ë“  ì‚¬ê±´ë“¤ì´ ì¼ì–´ë‚  í™•ë¥ ì€ 1ì…ë‹ˆë‹¤.
    - ë…ë¦½ì ì¸(ìƒí˜¸ë°°íƒ€ì ì¸) ì‚¬ê±´ ì¦‰ A or B í˜•íƒœì—ì„œ P(X1 U X2) = P(X1) + P(X2) â€»*(X1 âˆ© X2) = 0*    
- **Why probability?** 
    - ë¶ˆí™•ì‹¤í•œ ê´€ì°°(noise or error)
    - ë¶ˆì™„ì „í•œ ë°ì´í„°(unobservable or missing elements)
    - ë¶ˆì™„ë²½í•œ knowledge and model(over-simplified or incorrect)
- **Random variable**
    - íŠ¹ì • ìƒí™©ì„ ìˆ«ìë¡œ Mappingí•´ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
- **Probability mass function, pmf, í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜**
    - discrete random variable, ì´ì‚° í™•ë¥  ë³€ìˆ˜
- **Probability density function, pdf, í™•ë¥  ë°€ë„ í•¨ìˆ˜**
    - continuous random variables, ì—°ì† í™•ë¥  ë³€ìˆ˜
- **Cumulative distribution function, cdf, ëˆ„ì  ë¶„í¬ í•¨ìˆ˜**
    - None-decreasing, right-continuous
- **Joint probability, ê²°í•© í™•ë¥ **
    - Xì™€ Yë¼ëŠ” ì´ë²¤íŠ¸ê°€ ìˆì„ ë•Œ, ë™ì‹œì— ì¼ì–´ë‚  í™•ë¥  `P(X,Y) or P(Xâˆ©Y)`
    - cf. Independence : P(X, Y) = P(X)P(Y)
- **Conditional probability, ì¡°ê±´ë¶€ í™•ë¥ **
    - Xê°€ ì¼ì–´ë‚  ë•Œ Yê°€ ì¼ì–´ë‚  í™•ë¥ , P(Y|X) = P(Y, X)/P(X)
    - cf. Independence : P(Y|X) = P(Y)
    - **Chain rule** : ì—°ì‡„ì  ì‚¬ê±´ì´ ë°œìƒí•  ë•Œ, ê¼¬ë¦¬ì— ê¼¬ë¦¬ë¥¼ ë­…ë‹ˆë‹¤.<br />
        `P(X, Y)=P(Y|X)P(X) / P(X3, X2, X1)=P(X3|X2, X1)P(X2|X1)P(X1)`
    - **Bayes's theorem** : <br />
        posterior`P(Y|X)`= likelihood`P(X|Y)`prior`P(Y)`/marginalization`P(X)`
    - **Marginal probability**
        ![gd](./images/marginal.png)
    - **Expectation, ê¸°ëŒ“ê°’** : Weighted Average, ê°€ì¤‘ í‰ê· 
        - Alias : Mean, average, the first moment
        - cf. Arithmetic mean, ì‚°ìˆ  í‰ê·  : ê° ìš”ì†Œë“¤ì´ ê· ì¼í•˜ê²Œ ë°œìƒí•©ë‹ˆë‹¤.
        - Properties
            - Linearity : E[X + Y] = E[X] + E[Y] and E[aX] = aE[X]
            - Non-multiplicativity: E[XY] != E[X]E[Y] <br />
                `cf. If X and Y are independent, E[XY] = E[X]E[Y]`
    - **Variance** : í•œ ê°œì˜ random variableì— í•´ë‹¹í•©ë‹ˆë‹¤.
        - Alias : The second central moment.
        - Calculation : Var(X) = E[X<sup>2</sup>] - E[X]<sup>2</sup>
        - cf. Covariance : 2ê°œ ì´ìƒì˜ random variableì— í•´ë‹¹í•©ë‹ˆë‹¤.<br />
            Cov(X, Y) = E[(X-E[X])(Y-E[Y])]<br />
            Var(X) = Cov(X, X)
        - Properties
            - Var(X) >= 0 : Non-negative
            - Var(X+a) = Var(X) : Invariant to a location parameter
            - Var(aX) = a<sup>2</sup>Var(X) : Squared scale
            - Var(aX + bY) = a<sup>2</sup>Var(X) + b<sup>2</sup>Var(Y) + 2abCov(X, Y)
<br />

## Information Theory_Entropy
- **(Shannon)Information** : I(x) = -log<sub>2</sub>P(x) <br />
    ì˜ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ” ì‚¬ê±´(unlikely event)ì˜ ì •ë³´ëŠ” ìì£¼ ë°œìƒí• ë§Œí•œ ì‚¬ê±´ë³´ë‹¤ ì •ë³´ëŸ‰ì´ ë§ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. 
    - Alias : Surprisal, information content
- **Entropy**
    - Shannon informationì˜ í‰ê·  or 'average of surprise' or 'average of the number of bits'ì…ë‹ˆë‹¤.
    - **Bernoulli distribution** : ë™ì „ì„ ë˜ì§ˆ ë•Œ
    ![gd](./images/bernoulli.png)
