<p align="center" style="font-size:50px">
    <a href="https://github.com/lsw6684/ComputerScience">HOME</a>
</p>

***

# Reinforcement Learning
- [Introduction to Reinforcement Learning](#introduction-to-reinforcement-learning)
- [OpenAI and TensorFlow(with Docker)](#openai-and-tensorflowwith-docker)
- [Markov Decision Process and Dynamic Programming](#markov-decision-process-and-dynamic-programming)
- [Monte Carlo Methods](#monte-carlo-methods)

<br />

## Introduction to Reinforcement Learning
- **ML : Machine Learning**
    - í”„ë¡œê·¸ë¨ì´ ë°ì´í„°ë¡œë¶€í„° ìë™ìœ¼ë¡œ í•™ìŠµì„ í•©ë‹ˆë‹¤.
    - ë°ì´í„°ì˜ ì¼ë¶€ Exampleë“¤ì„ ì¼ë°˜í™”ì—¬ íŠ¹ì • ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
    - ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ Training dataë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
- **ML ì•Œê³ ë¦¬ì¦˜ì˜ ì¢…ë¥˜**
    - **ì§€ë„ í•™ìŠµ Supervised Learning** : **input(X) & desired output(Y)** ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í•™ì  **ëª¨ë¸(f)** ì„ ë§Œë“­ë‹ˆë‹¤.
        - Regression : Data pointë¥¼ Generalization í•˜ì—¬ ax + b í˜•íƒœì˜ í•¨ìˆ˜ f, **ì—°ì†ì ì¸** Function lineì„ ë§Œë“¤ì–´ **real value**ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        - Classification : Inputì´ **ì–´ë–¤ class**ì— ì†í•˜ëŠ”ì§€ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - **ë¹„ì§€ë„ í•™ìŠµ Unsupervised Learning** : ë°ì´í„°ì— input(X)ë§Œì´ ìˆìœ¼ë©° **desired output(Y)ì´ ì—†ìŠµë‹ˆë‹¤**. ë°ì´í„°ì˜ íŒ¨í„´, êµ¬ì¡°ë¥¼ ì°¾ì•„ë‚´ì–´ ê·¸ë£¹í•‘ì´ë‚˜ **í´ëŸ¬ìŠ¤í„°ë§**ì„ í•©ë‹ˆë‹¤.
        - desired outputì´ ì—†ê¸° ë•Œë¬¸ì— y=f(x) ë¶ˆê°€ëŠ¥.

    - **ê°•í™” í•™ìŠµ Reinforcement Learning** : íŠ¹ì • í™˜ê²½ì—ì„œ Agent(SW or HW)ê°€ í˜„ì¬ì˜ ìƒíƒœë¥¼ ì¸ì‹í•˜ì—¬, ëˆ„ì ëœ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” í–‰ë™ í˜¹ì€ í–‰ë™ ìˆœì„œë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
        - ë°ì´í„°ë¼ëŠ” í‘œí˜„ì´ ì¡´ì¬í•˜ì§€ ì•Šê³  íŠ¹ì • Agentì˜ í–‰ë™ì— ë”°ë¼ Rewardë¥¼ maximizeí•˜ì—¬ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        - **Trial and Error**, ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ì—ëŸ¬ë¥¼ ì¤„ì´ê³  Rewardë¥¼ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.
        - There might be **delayed rewards** - Positive rewardê°€ ë‚˜ì˜¬ë§Œí•œ actionì´ ì¦‰ê°ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šìœ¼ë©´ ì§€ì—°ë©ë‹ˆë‹¤.
        - ex) Dog 
            ```
            ê°•ì•„ì§€(Agent)ì—ê²Œ ìºì¹˜ë³¼ í•˜ëŠ” ê²ƒì„ ê°€ë¥´ì¹©ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ê°€ë¥´ì¹  ìˆ˜ ì—†ìœ¼ë‹ˆ ê³µì„ ë˜ì§€ê³  ë‹¤ì‹œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì„±ê³µí•  ë•Œë§ˆë‹¤ ê°„ì‹(Positive Reward)ì„ ì¤ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì‹¤íŒ¨ í•˜ë©´ ê°„ì‹ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤(Negative Reward). ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´, ê°•ì•„ì§€ëŠ” ì–´ë–¤ Actionì„ í•´ì•¼ ê°„ì‹ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ì•Œê²Œ ë©ë‹ˆë‹¤.
            ```
        - Agentê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 2ê°€ì§€ ì „ëµ
        1. Positive rewardë¥¼ ì–»ê¸° ìœ„í•´ **ë‹¤ë¥¸ Actionì„ íƒí—˜**í•©ë‹ˆë‹¤.
        2. ì´ì „ì˜ Actionë“¤ ì¤‘ì— **positive rewardê°€ ìˆë˜ í–‰ë™ì— ì§‘ì°©**í•©ë‹ˆë‹¤.
            ```
            ì¥ë‹¨ì ì´ ê³µì¡´í•˜ë¯€ë¡œ Trade-offê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
            ë‘ ê°€ì§€ ì „ëµì€ ì ë‹¹í•œ ë¹„ìœ¨ë¡œ ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            ```
        - **exploration(íƒí—˜)ê³¼ exploitation(í™œìš©)ì—” trade-offê°€ ìˆìœ¼ë©° ë™ì‹œì— ë‘˜ ë‹¤ ì‹œí–‰í•  ìˆ˜ ì—†ì–´ì„œ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤.**
- **Typical RL Algorithm**
1. Agentê°€ í™˜ê²½ê³¼ Interaction í•˜ì—¬ Actionì„ ì·¨í•©ë‹ˆë‹¤.
2. Agentê°€ Another stateë¡œ ì›€ì§ì…ë‹ˆë‹¤.
3. ìˆ˜í–‰í•œ Actionê³¼ stateë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • Rewardë¥¼ ë°›ìŠµë‹ˆë‹¤.
4. ì§€ê¸ˆê¹Œì§€ì˜ Rewardë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–¤ Actionì´ ì¢‹ê³  ë‚˜ìœì§€ ì´í•´í•©ë‹ˆë‹¤.
5. ì¢‹ì€ Actionì´ ìˆë‹¤ë©´ ê·¸ í–‰ë™ì„ Exploitationí•˜ê³  Another action ì¦‰, Explorationì„ ì‹œë„í•©ë‹ˆë‹¤.
`AgentëŠ” ìƒìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ê°•í™”ë©ë‹ˆë‹¤.`

- **RLì´ MLë“¤ê³¼ ë‹¤ë¥¸ ì **
    - Compared to **Supervised Learnning**
        - ì§€ë„í•™ìŠµ(Supervised Learning)ì€ Machine(agent)ì´ training dataë¡œë¶€í„° ë¬´ì–¸ê°€(íŒ¨í„´)ë¥¼ ë°°ì›ë‹ˆë‹¤. Learning Dataë¡œë¶€í„° ì •ê·œí™”(normalization)í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ inputê³¼ output ì¦‰, ì •ë‹µê°’(label)ì„ ê°€ì§€ê³  ìˆëŠ”  dataë¥¼ ê°€ì§„ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ë³¸ì  ì—†ëŠ” ë°ì´í„°(unseen data)ë¥¼ ì ìš©í–ˆì„ ë•Œ ì˜¬ë°”ë¥¸ ê²°ê³¼ì— ê·¼ì ‘í•  ìˆ˜ ìˆë„ë¡ íŠ¹ì • í™˜ê²½ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì§€ì‹ì„ ë³´ìœ í•œ External Supervisorê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì¦‰, **Supervised Learningì€ ëª…í™•í•œ ì§€í‘œê°€ ìˆìŠµë‹ˆë‹¤.**
    - Compared to **Unsupervised Learning**
        - ëª¨ë¸ì— training dataë¥¼ ì¤€ë‹¤ëŠ” ì ì€ Supervised Learningê³¼ ê°™ì§€ë§Œ Unsupervised Learningì€ output ì—†ì´ inputë§Œ ì „ë‹¬í•˜ê³  **inputì— í¬í•¨ëœ hidden structure**ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  **RLì˜ Modelì€ rewardë¥¼ maximizing**í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. `RLì€ ì§€ì†ì  í”¼ë“œë°±ì„ ìš”êµ¬í•©ë‹ˆë‹¤.`

<br />

- **Elements of RL**
    - **Agent**
        - RLë¡œ ë¬´ì–¸ê°€ë¥¼ ë°°ìš°ê³  ì§€ì ì¸ ê²°ì •ì„ ë§Œë“œëŠ” Learner, Software programì…ë‹ˆë‹¤.
        - í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš© í•¨ìœ¼ë¡œì¨ í–‰ë™ì„ ì·¨í•˜ë©°, ê·¸ í–‰ë™ìœ¼ë¡œë¶€í„° rewardë¥¼ ì·¨í•©ë‹ˆë‹¤.
        - ex) Super Mario ê²Œì„ì—ì„œì˜ Super Mario
    - **Policy function**
        - ì¼ì¢…ì˜ ê·œì¹™ìœ¼ë¡œ, íŠ¹ì • í™˜ê²½ì—ì„œ Agentê°€ í•˜ëŠ” í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.
        - **Lookup table** or **Search tree** í˜•ì‹ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        - Policyì˜ symbolì€ **ğ…**ì…ë‹ˆë‹¤.
        - ex) ì‚¬ë¬´ì‹¤ì—ì„œ ì§‘ìœ¼ë¡œ ê°€ëŠ” ê¸¸ì´ ìˆë‹¤.
            - ì§§ì€ ê¸¸ë“¤ê³¼ ìˆê³  ê¸´ ê¸¸ë“¤ì´ ìˆë‹¤.
            - ì—¬ê¸°ì„œ **ê¸¸**ì€ ì–´ëŠ ìª½ìœ¼ë¡œ ê°€ì•¼í•  ì§€ ì•Œë ¤ì£¼ëŠ” **Policy**ì…ë‹ˆë‹¤.
    - **Value Function**
        - Rewardë¥¼ ì˜ë¯¸í•˜ë©° Policyë§ˆë‹¤ Value Functionì„ ê°€ì§„ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - Value Functionì˜ symbolì€ **v(s)** ë¡œ stateì¸ së¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤. 
        - Agentê°€ Environmentì˜ íŠ¹ì • stateì— ìœ„ì¹˜í•  ë•Œ, ìµœì¢…ì ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” total expected rewardë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        - **Optimal Value Function**ì€ ëª¨ë“  statesì— ëŒ€í•˜ì—¬ ë”ìš± ì¢‹ì€ ê°’ì„ ê°€ì§€ëŠ” Value Functionì„ ìµœì ì˜ Value Function, Optimal Value Functionì´ë¼ ì¹­í•©ë‹ˆë‹¤.
        - RLì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” Optimal Value Functionì„ ê°–ëŠ” **Optimal Policy**ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    - **Model**
        - í™˜ê²½ì— ëŒ€í•œ Agentì˜ Representationì…ë‹ˆë‹¤.
        - RLì€ **Model-based learning** And **Model-free learning**, ì´ 2ê°€ì§€ íƒ€ì…ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
            - Model-based learning : AgentëŠ” Environmentì— ëŒ€í•œ Modelingì„ ê°–ê³  ìˆì„ ë•Œ, Modelì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • Actionì„ ì·¨í–ˆì„ ë•Œ ë‹¤ìŒ Stateê°€ ì–´ë–»ê²Œ ë ì§€ ì •í™•íˆ ì•Œê²Œ ë©ë‹ˆë‹¤.
            - Model-free learning : Agentê°€ Environmentì— ëŒ€í•œ ì •ë³´ê°€ ì „í˜€ ì—†ì„ ë•Œ, ë¬´ì‘ì • Actionì„ ì·¨í•˜ê³  Rewardë¥¼ ë°›ìŠµë‹ˆë‹¤. just **trail-and-error(ì‹œí–‰ì°©ì˜¤)** ì— ì˜ì¡´í•©ë‹ˆë‹¤.

- **Agent Environment Interface**
<p align="center"><img src="images/agentEnvironment.png" width="38%"></p>

```
Agent, A. íŠ¹ì • time state, t. Atê°€ Actionì„ í•˜ë©´
Environmentë¡œ ë¶€í„° Reward, Rtë¥¼ ë°›ê³  St+1ë¡œ Stateë¡œ ë°”ë€ë‹ˆë‹¤.
ìœ„ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
```
- **Model-free learning**ì„ í†µí•œ ì˜ˆì‹œ
<p align="center"><img src="images/maze.png" width="50%"></p>

- **RL Environmentì˜ ì¢…ë¥˜**, `2 ì¢…ë¥˜ì”© ë¹„êµ`
    - **â˜… Deterministic environment** - Agentê°€ ì·¨í•˜ëŠ” í˜„ì¬ Stateì— ëŒ€í•œ Actionì˜ ê²°ê³¼ê°€ ëª…í™•í•  ë•Œ, ë‹¤ìŒ Stateê°€ í™•ì •ì´ ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. `ex) ì²´ìŠ¤ê²Œì„ì—ì„œ playerì— ì˜í•œ outcomeì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`
    - **â˜… Stochastic environment** - ë‹¤ìŒ Stateê°€ ì–´ë–»ê²Œ ë ì§€ í™•ì •í•  ìˆ˜ ì—†ì„ ë•Œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. `ex) ì£¼ì‚¬ìœ„`
    - **â—† Fully observable environment** - RLì˜ ëª¨ë“  ì‹œê°„ ë™ì•ˆ Agentê°€ ê²°ì •í•  ìˆ˜ ìˆëŠ” Stateê°€ **ëª¨ë‘ ë³´ì—¬ì§‘ë‹ˆë‹¤.** Partially observable environmentì™€ ë¹„êµí•´ ë³´ìë©´, Fully observable environmentëŠ” Optimal Policyë¥¼ ì°¾ê¸° ì í•©í•©ë‹ˆë‹¤. `ex) ì²´ìŠ¤`
    - **â—† Partially observable environment** - ëª¨ë“  ì‹œê°„ ë™ì•ˆ Stateë¥¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `ex) í¬ì»¤ ê²Œì„`
    - **â–£ Discrete environment** - ë‹¤ë¥¸ Stateë¡œ ì´ë™ í•  Actionì˜ ìˆ˜ê°€ ìœ í•œí•  ë•Œë¥¼ ë§í•©ë‹ˆë‹¤. `ex) ì²´ìŠ¤ê²Œì„ì—ì„œ ë§ì´ ì›€ì§ì¼ ìˆ˜ ìˆëŠ” Actionì˜ ì¢…ë¥˜`
    - **â–£ Continuous environment** - ë‹¤ë¥¸ Stateë¡œ ì´ë™í•  Actionì˜ ìˆ˜ê°€ ë¬´í•œí•  ë•Œë¥¼ ë§í•©ë‹ˆë‹¤. `ex) self-driving car control`
    - **â–© Episodic and non-episodic environment**
        - Episode ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‚˜ë‰˜ëŠëƒ, ì•ˆë‚˜ë‰˜ëŠëƒë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.
        - Episodic environmentëŠ” non-sequential environmentë¡œë„ ë¶ˆë¦½ë‹ˆë‹¤.
        - Episodic environmentëŠ” Stateë§Œ ë°”ë€” ë¿ Actionê°„ì— ì„œë¡œ ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.(Independent)
        - non-episodic environmentëŠ” Sequential environmentë¡œë„ ë¶ˆë¦½ë‹ˆë‹¤.
        - Agentì˜ í˜„ì¬ í–‰ë™ì´ Stateë§Œ ë°”ë€” ë¿ ì•„ë‹ˆë¼ ë¯¸ë˜ì˜ Actionì—ë„ ì˜í–¥ì„ ë¼ì¹©ë‹ˆë‹¤. (dependent)
    - **â—ˆ Single and multi-agent environment**
        - Agent ê°œìˆ˜ê°€ 1ê°œëƒ ì•„ë‹ˆëƒê°€ ê¸°ì¤€ì…ë‹ˆë‹¤.
        - Multi-agent environmentê°€ ìƒëŒ€ì ìœ¼ë¡œ í›¨ì”¬ ë³µì¡í•©ë‹ˆë‹¤.
```
ì§ ì§€ì–´ì§„ environmentë¥¼ ì œì™¸í•˜ê³  ì—¬ëŸ¬ environmentë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ex)
1. Deterministic, Episodicí•œ Environment (OK)
2. Episodic, Non-episodicí•œ Environment (Invalid)
```
- **RL Platforms** - ì‹œë®¬ë˜ì´ì…˜, ë¹Œë”©, ë Œë”ë§, ê·¸ë¦¬ê³  Environmentì—ì„œ RL ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ### OpenAI Gym and OpenAI Universe
        - RL ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“¤ê³ , í‰ê°€í•˜ê³ , ë¹„êµí•  ìˆ˜ ìˆëŠ” Tool kitì…ë‹ˆë‹¤.
        - RLì€ MLì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤. Tensorflow, Theano, Keras ë“± ë‹¤ì–‘í•œ ML Frameworkë¡œ ì‘ì„±ëœ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ í˜¸í™˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - Agent Structureì— ëŒ€í•œ assumptionì´ ì—†ê¸° ë•Œë¬¸ì— ëª¨ë“  Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - OpenAI UniverseëŠ” OpenAI Gymì˜ í™•ì¥íŒì…ë‹ˆë‹¤.
        - êµ‰ì¥íˆ ë„“ì€ ë²”ìœ„ì˜ complex environment, ê°„ë‹¨í•œ ê²ƒë¶€í„° real-timeê¹Œì§€ trainingì‹œí‚¤ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - OpenAI UniverseëŠ” ê°€ìƒ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì„ ì´ìš© í•¨ìœ¼ë¡œì¨ ê¸°ì¡´ì— ì¡´ì¬í•˜ëŠ” ë‹¤ì–‘í•œ í”„ë¡œê·¸ë¨ê³¼ Gymì˜ connectionì„ ë³„ë‹¤ë¥¸ ì œì•½ ì—†ì´ ì‰½ê²Œ ì—°ë™í•´ ì¤ë‹ˆë‹¤. í™•ì¥ì„±ì´ êµ‰ì¥í•©ë‹ˆë‹¤.
        - Environment ììœ ë¡­ê²Œ êµ¬í˜„ ê°€ëŠ¥
    - **DeepMind Lap** - *ì•ŒíŒŒê³ *
        - fist-person 3D game platform, 1ì¸ì¹­ ì‹œì ì˜ 3Dê²Œì„í”Œë«í¼ìœ¼ë¡œ ê°•í™”í•™ìŠµ ê°œë°œê³¼ ì—°êµ¬ë¥¼ ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. - ***Customizable and Extndable***
        - Environment ììœ ë¡­ê²Œ êµ¬í˜„ ê°€ëŠ¥
    - **RL-Glue**
        - Agent, Environment, Programì´ ì„œë¡œ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ êµ¬í˜„ë˜ì–´ ìˆëŠ” ê²½ìš°ì—ë„ ì—°ê²°í•´ ì£¼ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        - ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ë§Œë“¤ì–´ ë†“ì€ ì‘ì—…ë¬¼ì„ ê³µìœ í•  ìˆ˜ ìˆì–´ì„œ Reusabilityê°€ ë†’ìŠµë‹ˆë‹¤.
    - **Project Malmo**
        - ë§ˆì¸í¬ë˜í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIì‹¤í—˜ í”Œë«í¼ì…ë‹ˆë‹¤.
        - ì‹œë‚˜ë¦¬ì˜¤ì˜ ì‹œê°„ì„ ë¹ ë¥´ê²Œ ì„¤ì •í•˜ì—¬ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ë§ˆì¸í¬ë˜í”„íŠ¸ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
    - **ViZDoom**
        - Multi-agents ì‹œìŠ¤í…œ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤.
        - Doom(1ì¸ì¹­ ìŠˆíŒ… ê²Œì„) environmentë§Œ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
- **Aplications of RL**
    - **Education** - personalized contentë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    - **Manufacturing** - ì‚°ì—…ìš© ë¡œë´‡, ë¡œë´‡ íŒ” ë“± intelligent robotìœ¼ë¡œ íŠ¹ì • ì„ë¬´ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ê°•í™”í•™ìŠµ ë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.
    -**Inventory management**- ê³µê¸‰ë§ Supply chain management, ìˆ˜ìš” ì˜ˆì¸¡ demand forecasting, warehouse operation ë“±ì„ ê°•í™”í•™ìŠµìœ¼ë¡œ ìµœì í™” ì‹œì¼œ ì´ìš©ë©ë‹ˆë‹¤. (ì „ë ¥ ì†Œë¹„ëŸ‰ ë“±)
    - **Finance** - ìƒì—…ì  ê±°ë˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    - **Natural language processing and Computer vision** - DLê³¼ ê²°í•©ëœ DRLì— ì‚¬ìš©ë˜ë©°, text ìš”ì•½, ì •ë³´ ì¶•ì•½, ê¸°ê³„ ë²ˆì—­(papago ë“±), ì´ë¯¸ì§€ ì¸ì‹ ë“±ì˜ ì •í™•ì„±ì„ ë†’íˆëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

## OpenAI Gym and TensorFlow(with Docker)
- **Docker**
    - virtual systemì˜ ì¼ì¢…ìœ¼ë¡œ, ì»¨í…Œì´ë„ˆì— ì†Œí”„íŠ¸ì›¨ì–´ë“¤ì´ íŒ¨í‚¤ì§•ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - ì»¨í…Œì´ë„ˆì—ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° í•„ìš”í•œ libraries, system tools, code, and runtime ë“±ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - Application ì‹¤í–‰ì— í•„ìš”í•œ ëª¨ë“  dependencyë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì‰½ê³  ë¹ ë¥´ê²Œ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - Docker Hubì—ì„œ Docker imageíŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ ì…‹íŒ…ì´ ë˜ì–´ìˆëŠ” Containerë¥¼ loadí•˜ì—¬ ì‰½ê³  ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        - **Container** - resource-isolatedì—ì„œ dependency ì—†ì´ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” **virtualization**ì…ë‹ˆë‹¤. Applicationì˜ ì½”ë“œ, dependencies ë“± ëª¨ë“  ê²ƒì„ í•˜ë‚˜ì˜ blockìœ¼ë¡œ íŒ¨í‚¤ì§• í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - **Container benefits**
        1. **Environment consistency** - portabilityê°€ ìˆê³  organizational and technical frictions of moving and applicationsë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì„œë²„ë¥¼ manually configuringí•˜ëŠ” ê²ƒ ì—†ì´ ìƒˆë¡œìš´ featuresë¥¼ ë¹ ë¥´ê²Œ releaseí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        2. **Operational efficiency** - ê°™ì€ ì„œë²„ì—ì„œ multiple applicationsë¥¼ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆì—ëŠ” ì •í™•í•˜ê²Œ ì¸¡ì •ëœ memory, disk space, and CPUê°€ ì •í•´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì‚¬ìš© ì‹œì—, êµ¬ì²´í™” í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤. ì¼ë°˜ virtual systemì€ Hardware levelë¶€í„° virtualizationì´ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ë§¤ìš° ëŠë¦¬ì§€ë§Œ, ContainerëŠ” í•˜ë‹¨ levelë¶€í„° virtualizationì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— **ë¶€íŒ…ì´ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.** **Scale-up and Scale-down**ì— ìš©ì´í•˜ë©° **Blue-Green deployment**íŒ¨í„´ê³¼ process isolationì„ ì œê³µí•©ë‹ˆë‹¤.

            `Blue-Green ë°°í¬ë€, ë¬´ì¤‘ë‹¨ ë°°í¬ Continuous Deliveryì…ë‹ˆë‹¤. êµ¬ ë²„ì „ê³¼ ìƒˆ ë²„ì „ì„ ë‚˜ë€íˆ êµ¬ì„±í•˜ê³  ë°°í¬ ì‹œì ì´ ë˜ë©´ íŠ¸ë˜í”½ì„ ì¼ì œíˆ ì „í™˜ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë²„ì „ ê´€ë¦¬ ë¬¸ì œë¥¼ ë°©ì§€í•  ìˆ˜ ìˆê³  ë¹ ë¥¸ ë¡¤ë°±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ì¥ì ìœ¼ë¡œ, ìš´ì˜ í™˜ê²½ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ìœ¼ë¡œ ìƒˆ ë²„ì „ í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. `
        3. **Developer Productivity** - independently upgrade each service
        4. **Version control** - Docker container images have a **manifest file**. ê·¸ë ‡ê¸° ë•Œë¬¸ì—, ë²„ì „ ì»¨íŠ¸ë¡¤ì„ í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.
    - **Container vs Virtual Machine**
    <p align="center"><img src="images/container_virtual.png" width="850"></p>
- [**OpenAI**](#openai-gym-and-openai-universe)
- **Tensorflow**
    - numerical computationì„ ë„ë¦¬ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Google open source software libraryì…ë‹ˆë‹¤.
    - DL, ML ë“±ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    - **data flow graph**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ í”Œë«í¼ì—ì„œë„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - multi-dimensional arrayë¥¼ ì§€ì›í•©ë‹ˆë‹¤.<br /><br />
    **Varialbes, Constants, Placeholders**
        ```python
        Variables : ê°’ì„ ì €ì¥í•˜ëŠ” Containerì…ë‹ˆë‹¤.
        - tf.Variable()
            >> weights = tf.Variable(tf.random_normal({3, 2}, stddev=0.1), name="weights")

        Constants : ìƒìˆ˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°’ì´ ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - tf.constant()
            >> x = tf.constant(13)

        Placeholders : variableê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤. data typeê³¼ dimension of arrayë¥¼ defineí•˜ê³  valueëŠ” assign í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, valueì—†ì´ ì •ì˜ í•˜ê³  íŠ¹ì • ë©”ëª¨ë¦¬ë§Œ ì¡ì•„ë‘” ë‹¤ìŒ runtimeì— ì‚¬ìš©ë©ë‹ˆë‹¤. 
        - tf.placeholder()
            >>> x = tf.placeholder("float", shape = None)
        # shapeì´ Noneìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì–´ë–¤ ì°¨ì›ì˜ arrayë„ ì‚½ì… ê°€ëŠ¥í•©ë‹ˆë‹¤.

        ğŸ˜ƒ Variableì€ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´, PlaceholderëŠ” external dataë¥¼ computational graphì— ì‚½ì…í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
        ```
    - **Computation Graph**
        - nodes(mathematical operations)ì™€ edges(tensors)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
        - **Tensorflowì—ì„œì˜ DL**ì€ multi-dimensional array. ì¦‰ Tensorë“¤ì´ mathematical operationsë¥¼ ê±°ì¹˜ë©´ì„œ ê²°ê³¼ë¥¼ ë‚´ê³  ê³„ì‚°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— Graphë¡œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒì€ êµ‰ì¥íˆ í•©ë¦¬ì ì¸ ì„ íƒì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - computation graphë¥¼ multi coreë¡œ ë…ë¦½ì ì¸(edgeë¡œ ì—°ê²°ë˜ì–´ ìˆì§€ ì•Šì€ ë…¸ë“œë“¤) ë¶„ë°° ê³„ì‚°ì„ í•˜ì—¬ ë¹ ë¥¸ ê³„ì‚°ì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤.
    - **Sessions**
        - ìƒìœ„ ì„¤ëª…í•œ ê³„ì‚°ë“¤ë¡œ defineí•œ ì½”ë“œë“¤ì„ ì—´ê³ , ì‹¤í–‰í•©ë‹ˆë‹¤.<br />
        `sess = tf.Session()`<br /> 
        ***And***<br />
        `sess.run()`
        ```python
        import tensorflow as tf
        a = tf.multiply(2, 3)
        print(a)
        # 6ì´ ë‚˜ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ aë¼ëŠ” ë…¸ë“œì˜ ì£¼ì†Œ ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤.

        import tensrflow as tf
        a = tf.multiply(2, 3)
        with tf.Session() as sess:
            print(sess.run(a))       # ì„¸ì…˜ ì‹¤í–‰
        ```
- **TensorBoard** : TensorFlowì˜ ê°€ìƒí™”ë¡œ ê³„ì‚° ê³¼ì •ì¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
    - **Adding Scope** : ê³„ì‚°ì„ ê·¸ë£¹í•‘í•˜ì—¬ ë…¸ë“œë“¤ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ë³µì¡ì„±ì´ ì¤„ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ TensorBoardë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br />
    `tf.name_scope()`

<br />

## Markov Decision Process and Dynamic Programming
- Markov Chain and Markov Process
    - Markov property : ë¯¸ë˜ëŠ” ì˜¤ì§ í˜„ì¬ì— ì˜í•˜ë©°, ê·¸ ì´ìƒì˜ ê³¼ê±°ì—ëŠ” ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤. <br />`t >> t+1 (O), t-1 >> t+1 (X)`
    - Markov chain
        - í˜„ì¬ stateì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹¤ìŒ stateë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. not previous states
        - ë¯¸ë˜ëŠ” ê³¼ê±°ì— ëŒ€í•˜ì—¬ conditionally independentí•©ë‹ˆë‹¤.
        - decision-making processê°€ ì•„ë‹ˆë¼, ë‹¨ìˆœíˆ Markov propertyë¥¼ ë”°ë¥´ëŠ” í™•ë¥  ëª¨ë¸ì„ í‘œí˜„í•©ë‹ˆë‹¤.
            ```
            ì§€ê¸ˆ êµ¬ë¦„ì´ ë¼ë©´ ë¹„ê°€ ì˜¬ ìˆ˜ ìˆì§€ë§Œ, ê³¼ê±°ì— êµ¬ë¦„ì´ ìˆë‹¤ê³  í•´ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.
            ```
        - **Transition** : ë‹¤ìŒ stateë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
        - **Transition probability** : ë‹¤ìŒ stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì…ë‹ˆë‹¤.
        ![gd](./images/transition.png)
        - **Markov chain in the form a state diagram with transition probability**
        ![gd](./images/markov_diagram.png)
    - **Markov Decision Process, MDP**
        - Markov chainì˜ í™•ì¥íŒì…ë‹ˆë‹¤.
        - decision-makingì˜ frameworkë¥¼ Mathematicí•˜ê²Œ modelingí•©ë‹ˆë‹¤.
        - ëŒ€ë¶€ë¶„ì˜ RL problemì€ MDPë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - **Five important elements to represent MDP**
        1. **States, (S)** : Agentê°€ ì‹¤ì œë¡œ ë¬´ì–¸ê°€ë¥¼ í•˜ëŠ” í™˜ê²½ì˜ ì§‘í•©ì…ë‹ˆë‹¤.
        2. **Actions, (A)** : Agentê°€ í•˜ëŠ” í–‰ë™ì— ëŒ€í•œ ì§‘í•©ì…ë‹ˆë‹¤.
        3. **Transition probability, (P <sup>a</sup><sub>ss'</sub>)** state (s)ì—ì„œ another state (s')ë¡œ ì´ë™í•˜ë©´ì„œ action (a)ë¥¼ í•¨ìœ¼ë¡œì¨ sì—ì„œ s'ë¡œ transitionë  í™•ë¥ ì…ë‹ˆë‹¤.<br />
        P <sup>a</sup><sub>ss'</sub> = pr(S <sub>t+1</sub> = s' | s <sub>t</sub> = s, a <sub>t</sub> = a)
        4. **Reward probability, (R <sup>a</sup><sub>ss'</sub>)** : S ì—ì„œ another state s'ë¡œ ì´ë™í•˜ê¸° ìœ„í•´ aë¥¼ ìˆ˜í–‰í–ˆì„ ë•Œ ë°›ì„ ìˆ˜ ìˆëŠ” Rewardì˜ í™•ë¥ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. <br />
        R <sup>a</sup><sub>ss'</sub> = E(R <sub>t+1</sub> | s <sub>t</sub> = s, s <sub>t+1</sub> = s', a <sub>t</sub> = a)
        5. ### Discount factor, (Î³)
            (Î³) ë‹¹ì¥ì˜ Rewardì™€ ë¯¸ë˜ì˜ Reward ì‚¬ì´ì— importanceë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. <br />
            Î³ë¥¼ ê³±í•´ì¤Œìœ¼ë¡œì¨ Episodic task, Continuous task ëª¨ë‘ì— ëŒ€í•˜ì—¬ unified <br /> `Hyperparameter - ê¸°ê³„í•™ìŠµì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°` <br />
            Immediate rewardë³´ë‹¤ Future rewardì˜ ì¤‘ìš”ì„±ì„ ë³´ì¥í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•©ë‹ˆë‹¤.
            ![gd](./images/discount_factor.png)

            

- **Rewards and Returns**
![gd](./images/rewards_returns.png)

- **Episodic and Continuous Tasks**
    - **Episodic task**
        - ëì´ ì¡´ì¬í•˜ëŠ” task(end - terminal state)
        - In RL, EpisodeëŠ” agentì™€ environment ì‚¬ì´ì˜ interactionìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `interaction : from initial to final states(end)`
        - ì‹œì‘ ì ê³¼ ë ì ì˜ ê²½ê³„ ê°€ ëª…í™•í•˜ê³  ê° Episodeê°€ ë…ë¦½ì ìœ¼ë¡œ ì „í˜€ ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - **Continuous Tasks**
        - ëë‚˜ì§€ ì•ŠëŠ” task(there is not a terminal state)
        - R <sub>t</sub>ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë¬´í•œì •ìœ¼ë¡œ ë”í•´ì•¼ í•˜ê¸° ë•Œë¬¸)
        - ì¦‰, Rewardë¥¼ maximizeí•˜ëŠ” ë°©ë²•ì´ ì—†ìœ¼ë¯€ë¡œ [Discount factor](#discount-factor-Î³)ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.
- **Policy Function(Ï€)**
    - Ï€(s) : S â†’ A, íŠ¹ì • stateì—ì„œ ì–´ë–¤ actionì„ ì·¨í•  ***í™•ë¥ ***ì…ë‹ˆë‹¤.
- **Value Function, V(s)***(=State Value Function)*
    - Agentê°€ policy Ï€ì— ê¸°ë°˜í•˜ì—¬ íŠ¹ì • stateì— ìˆì„ ë•Œ, í•´ë‹¹ Agentê°€ ë¨¸ë¬¼ê¸° ì¢‹ì€ ì •ë„ë¥¼ ì˜ë¯¸í•˜ëŠ” return ê°’ì…ë‹ˆë‹¤.
    ![gd](./images/value_function.png)
- **State-Action Value Function (=Q function), Q(s, a)**
    - policy Ï€ì— ê¸°ë°˜í•˜ì—¬ íŠ¹ì • stateì—ì„œ ì–´ë–¤ Actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ í‘œí˜„í•˜ëŠ” return ê°’ì…ë‹ˆë‹¤.
    ![gd](./images/savf.png)
```
Value Funftionì´ íŠ¹ì • stateì— ìˆëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‘œí˜„í•˜ëŠ” ê²ƒì´ë¼ë©´
Q functionì€ í•´ë‹¹ stateì—ì„œ íŠ¹ì • actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‘œí˜„í•©ë‹ˆë‹¤.
```
- **Recursive Relationships in Value Function**
![gd](./images/recursive.png) 

<br />

- **Bellman Equation and Optimality**
    - MDPë¥¼ í’€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `= Optimal policyì™€ Optimal value functionì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`
    - Policyê°€ ë°”ë€Œë©´ value functionë„ ë°”ë€ë‹ˆë‹¤.
    - Optimal value function V*(s)ëŠ” ëª¨ë“  stateì—ì„œ ë‹¤ë¥¸ value functionë³´ë‹¤ ë” í° valueë¥¼ ê°€ì§‘ë‹ˆë‹¤. `Optimal = *`<br /> 
    ![gd](./images/v_star.png) - V <sup>Ï€</sup> (s)ê°€ ìµœëŒ€ì˜ Së¥¼ ê°€ì§ˆ ë•Œ Vê°’ì„ ë§í•©ë‹ˆë‹¤. 
    - V*(s)ê°€ Maximum returnì´ê¸° ë•Œë¬¸ì— Maximum Q functionë„ ë©ë‹ˆë‹¤. <br />
    ![gd](./images/v_start2.png)
```
Expectation(E), ê¸°ëŒ“ê°’
- í™•ë¥ ë³€ìˆ˜ì˜ ê¸°ëŒ“ê°’ì€ ê° ì‚¬ê±´ì´ ë²Œì–´ì¡Œì„ ë•Œì˜ ì´ë“ê³¼ ê·¸ ì‚¬ê±´ì´ ë²Œì–´ì§ˆ í™•ë¥ ì„ ê³±í•œ ê²ƒì„ ì „ì²´ ì‚¬ê±´ì— ëŒ€í•´ í•©í•œ ê°’ì…ë‹ˆë‹¤.
- ì„ í˜• ì—°ì‚°ìì´ë©° ê°€ì‚°ì„±, ë™ì°¨ì„±ì´ ì„±ë¦½í•©ë‹ˆë‹¤.
    >>> E(X + Y) = E(X) + E(Y)
    >>> E(cX) = cE(X)
ex) ì£¼ì‚¬ìœ„ê°’ì˜ ê¸°ëŒ“ê°’
1*1/6 + 2*1/6 ... 6*1/6 = 3.5
```
`MDPë¥¼ í‘¼ë‹¤ëŠ” ê²ƒì€ Bellman Equationì„ ì´ìš©í•˜ì—¬ Optimal policyë¥¼ ì°¾ëŠ” ê²ƒì„ ì˜ë¯¸`

<br />

- **Dynamic Programming(DP)**
    - MDPë¡œ ì •ì˜ëœ Environmentì˜ perfect modelì— ëŒ€í•´ Optimal policyë¥¼ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
        - Perfect modelì´ ì£¼ì–´ì¡Œë‹¤? <br />
        í•´ë‹¹ MDPì— ëŒ€í•˜ì—¬ state, action, rewardë¥¼ ì•Œê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
    - Bellman Optimality equationì„ ì•Œì•„ëƒ…ë‹ˆë‹¤.
    - Bellman equationì„ í•´ê²°í•˜ëŠ” 2ê°€ì§€ ë°©ë²•
        - [Value iteration](#value-iteration)
        - [Policy iteration](#policy-iteration)

    <br />

    - ### Value Iteration
    1. Valueë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°˜ë³µí•©ë‹ˆë‹¤. 
    2. random value functionìœ¼ë¡œ ì‹œì‘í•˜ì—¬(`ex: ëª¨ë“  stateì— ëŒ€í•œ valueë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ì‹œì‘`) optimal value functionì´ ë  ë•Œê¹Œì§€ ìƒˆë¡œìš´ improved value functionì„ ë°˜ë³µ(ìˆœí™˜)í•˜ì—¬ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        - Optimalì€ ì•„ë‹ˆê² ì§€ë§Œ, V(s)ê°€ ë„ì¶œë©ë‹ˆë‹¤. í˜„ì¬ V(s)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  state action pairì— ëŒ€í•˜ì—¬ Q functionì„ ê³„ì‚°í•˜ê³  íŠ¹ì • stateì— max valueë¥¼ ê°€ì§€ëŠ” Qê°’ì„ í•´ë‹¹ stateì˜ valueë¡œ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.
    3. í˜„ì¬ value functionì´ ì´ì „ iterationì˜ value functionì˜ ì°¨ì´(change in the value between each iteration)ê°€ ì‘ì„ ë•Œ ê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
        - ì°¾ì•„ë‚¸ optimal value functionìœ¼ë¡œ optimal policyë¥¼ ì‰½ê²Œ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ![gd](./images/VI1.png)

    - ### Policy Iteration
    1. random policyë¡œ ì‹œì‘í•˜ì—¬ í•´ë‹¹ policyì˜ value functionì„ ì°¾ì•„ë‚´ëŠ” ë°©ì‹ìœ¼ë¡œ iterationí•©ë‹ˆë‹¤.
    2. optimal value functionì´ ì•„ë‹ˆë©´ new improved policyë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    3. ìƒìœ„ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    
        **Policy iterationì˜ 2ê°€ì§€ steps**
        - **Policy evaluation** : policyì˜ value functionì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        - **Policy improvement** : optimal value functionì´ ì•„ë‹ˆë¼ë©´ new improved policyë¥¼ ì°¾ìŠµë‹ˆë‹¤. <br />

            ![gd](./images/PI1.png)

<br />

## Monte Carlo Methods
ê³µí•™ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ìœ¼ë¡œ RLì— ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í™˜ê²½ì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ë¥¼ ë•Œ, model dynamics(transition probaility)ë¥¼ ëª¨ë¥¼ ê²½ìš°, ëª¨ì§‘ë‹¨ì—ì„œ random samplingí•œ í‘œë³¸ì§‘ë‹¨ìœ¼ë¡œ approximate solutionì„ ì°¾ì•„ëƒ„ìœ¼ë¡œ statistical techniqueì…ë‹ˆë‹¤.
- random samplingì„ í†µí•´ approximates(ê·¼ì‚¬ì¹˜)ë¥¼ ì°¾ì•„ë‚´ë©° ì‹¤í–‰ íšŸìˆ˜ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ optimal solutionê³¼ ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.
- terminal state(ë ì )ê°€ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— episodic taskì—ë§Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- model ì—†ì´ë„ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— **model-free learning algorithm**ìœ¼ë¡œë„ ë¶ˆë¦½ë‹ˆë‹¤. `DPëŠ” model based learning algorithm`
- expected returnì´ ì•„ë‹Œ episodeì—ì„œ ë‚˜ì˜¤ëŠ” í‰ê·  ê°’(mean return)ì„ êµ¬í•¨ìœ¼ë¡œì¨ stateì˜ value functionì„ approximationí•©ë‹ˆë‹¤.
- ê³¼ì •
1. value functionì„ estimationí•˜ëŠ” ê²ƒì´ ëª©ì ì´ë¯€ë¡œ value functionì„ random ê°’ìœ¼ë¡œ initialize ì‹œí‚µë‹ˆë‹¤.
2. returnë“¤ì„ ì €ì¥ í•  empty listë¥¼ ë§Œë“­ë‹ˆë‹¤.
3. episodeë¥¼ ìƒì„±í•˜ê³  í•´ë‹¹ episodeì˜ statesì— ëŒ€í•œ returnì„ ê³„ì‚°í•˜ì—¬
4. empty listì— appendí•©ë‹ˆë‹¤.
5. returnì˜ í‰ê· ì„ êµ¬í•˜ê³  value functionì„ updateí•©ë‹ˆë‹¤.
<p align="center"><img src="images/mrp.png" width="300"></p>

- **Two types of Monte Carlo Prediction**
    - **First visit Monte Carlo** <br />
        - í•˜ë‚˜ì˜ episodeì—ì„œ ê°™ì€ stateì— ì—¬ëŸ¬ ë²ˆ ë°©ë¬¸í•  ìˆ˜ ìˆì§€ë§Œ, ì²« ë²ˆì§¸ ë°©ë¬¸ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤.
        ![gd](./images/fv.png)
        
    - **Every visit Monte Carlo**
        - ì²« ë²ˆì§¸ ë°©ë¬¸ë§Œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ states ë°©ë¬¸ì— ëŒ€í•œ ëª¨ë“  valueë¥¼ ê°ê° ì‚¬ìš©í•˜ì—¬ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.<br />

    ë‘ MC ëª¨ë‘ state Sì— visití•˜ëŠ” íšŸìˆ˜ê°€ ë¬´í•œëŒ€ë¡œ ê°ˆ ìˆ˜ë¡ V<sup>Ï€</sup>(s)ì— convergeí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ ë¬´í•œëŒ€ë§Œí¼ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— **í•´ë‹¹ stateì— ì—…ë°ì´íŠ¸ ë˜ëŠ” value functionì´** ì´ì „ ë£¨í”„ì—ì„œì˜ ê°’ê³¼ **ì°¨ì´ê°€ ì‘ë‹¤ë©´ converge**í•œ ê²ƒìœ¼ë¡œ ë´…ë‹ˆë‹¤.
    - **converge í•˜ëŠ” ì´ìœ **
        - ê°ê°ì˜ returnì€ independent(episodeë¼ë¦¬ ë…ë¦½ì´ë¼)í•˜ê³  ìœ í•œ ë¶„ì‚°ì„ ê°€ì§€ëŠ” V<sup>Ï€</sup>(s)ì´ë‹¤(IID).
        - **The law of large numbers, í° ìˆ˜ì˜ ë²•ì¹™**ì„ ë”°ë¦…ë‹ˆë‹¤.<br />
            `í° ëª¨ì§‘ë‹¨ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë½‘ì€ í‘œë³¸ì˜ í‰ê· ì€ ëª¨ì§‘ë‹¨ì˜ í‰ê· ê³¼ ê°€ê¹Œìš¸ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.`
- **Monte Carlo Control**<br />cc
    DPì˜ Policy iterationì²˜ëŸ¼ Policy evaluationê³¼ Policy improvementë¥¼ ë°˜ë³µí•˜ì—¬ Optimal policyë¥¼ ì°¾ìŠµë‹ˆë‹¤. PolicyëŠ” ì—…ë°ì´íŠ¸ëœ value functionìœ¼ë¡œ improveë˜ê³  value functionì€ ìƒˆë¡­ê²Œ ê³„ì‚°ëœ Policyë¡œ ë‹¤ì‹œ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤.
    ![gd](./images/mcc.png)
- **Monte Carlo Exploration Starts**
    ê° ì—í”¼ì†Œë“œë§ˆë‹¤ ì„ì˜ì˜ ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ íƒí—˜ì ìœ¼ë¡œ í–‰ë™ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ë§ì€ ì—í”¼ì†Œë“œê°€ ìˆì„ ê²½ìš°, ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ëª¨ë“  ìƒíƒœë¥¼ í¬ê´„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²ƒì„ **MC-ES**ë¼ê³  í•©ë‹ˆë‹¤. <br />

    **MC-ES ìˆœì„œ**
    1. Q functionê³¼ Policyë¥¼ Random valueë¡œ ì´ˆê¸°í™”í•˜ê³  returnì„ ì €ì¥í•  empty listë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    2. ì„ì˜ë¡œ ì´ˆê¸°í™”ëœ ì •ì±…ìœ¼ë¡œ ì—í”¼ì†Œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
    3. ì—í”¼ì†Œë“œì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ê³ ìœ í•œ ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•œ ê³„ì‚°ì„ í•˜ê³  return ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    4. í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œì— ë™ì¼í•œ ìƒíƒœ-í–‰ë™ ìŒì´ ì—¬ëŸ¬ë²ˆ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ê³ ìœ í•œ ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.
    5. return ë¦¬ìŠ¤íŠ¸ì—ì„œ í‰ê· ì„ ì·¨í•´ ê·¸ ê°’ì„ ê·œí•¨ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
    6. ìµœì ì˜ Policyë¥¼ ì„ íƒí•˜ê³  ê·¸ ìƒíƒœì—ì„œ ìµœëŒ€ Q(s, a)ë¥¼ ê°–ëŠ” í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤.
    7. ëª¨ë“  statesì™€ action ìŒì„ í¬ê´„í•˜ë„ë¡ ìœ„ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    ![gd](./images/mces.png)

- **On-policy / Off-policy MC Control** <br />
MC-ESì—ì„œ ëª¨ë“  combinationì˜ statesì™€ actionsë¥¼ exploreí•˜ê¸°ì—” ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í˜„ì‹¤ì ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•´ 2 ê°€ì§€ control algorithmì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - **In On-policy MC control, Îµ-greedy policy or Îµ-soft policy**
        - 1-Îµì˜ í™•ë¥ ë¡œ íƒí—˜ ì—†ì´ í˜„ì¬ ìƒíƒœì—ì„œ ìµœì„ ì˜ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤.
    - **Off-policy MC control**
        - **Target policy** : í•™ìŠµì„ í•˜ì—¬ Optimal policyê°€ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        - **Behavior policy** : í–‰ë™ì„ generationí•  ë•Œ ì“°ì´ë©° ê°€ëŠ¥í•œ ëª¨ë“  possible statesì™€ actionì„ exploreí•©ë‹ˆë‹¤.
    - **Importance sampling** : ë‹¤ë¥¸ distributionì˜ sampleì—ì„œ í‘œë³¸ì„ ì¶”ì¶œí•˜ê³  ì›ë˜ì˜ ëª¨ì§‘ë‹¨ valueë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        - **Ordinary importance sampling**
        - **Weighted importance sampling**